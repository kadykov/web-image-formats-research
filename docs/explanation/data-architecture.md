---
title: "Data architecture and configuration"
description: "How datasets, preprocessing, encoded outputs and configuration files are organized for the pipeline."
---

This document explains the architectural decisions behind the project's data and configuration organization.

## Directory Structure

```text
web-image-formats-research/
├── config/                   # Configuration files
│   ├── datasets.json         # Dataset definitions
│   └── datasets.schema.json  # Dataset config schema
│
├── data/                     # All research data
│   ├── datasets/             # Raw datasets (git-ignored)
│   ├── preprocessed/         # Preprocessed images (git-ignored)
│   ├── encoded/              # Encoded images (git-ignored)
│   ├── metrics/              # Quality measurements (git-ignored)
│   └── analysis/             # Analysis outputs (git-ignored)
│
├── src/                      # Source code
│   ├── dataset.py            # Dataset fetching
│   ├── preprocessing.py      # Image preprocessing
│   ├── encoder.py            # Format encoding
│   ├── quality.py            # Quality measurement
│   └── analysis.py           # Analysis and visualization
│
├── scripts/                  # Executable scripts
│   └── fetch_dataset.py      # Dataset fetching CLI
│
├── tests/                    # Test suite
└── docs/                     # Documentation
```

## Design Principles

### 1. Separation of Data and Code

**Data** (`data/`) and **code** (`src/`, `scripts/`) are strictly separated:

- Code is version-controlled
- Data is git-ignored (except README files)
- Large datasets don't bloat the repository
- Easy to share code without data

### 2. Configuration-Driven Pipeline

All pipeline parameters live in **configuration files** (`config/`):

- `datasets.json` - What datasets to fetch
- Future: `preprocessing.json` - How to preprocess
- Future: `encoding.json` - Encoding parameters
- Future: `quality.json` - Quality measurement settings
- Future: `analysis.json` - Analysis parameters

**Benefits:**

- Version-controlled configurations
- Easy parameter tuning without code changes
- Reproducible experiments
- Schema validation (JSON Schema)

### 3. Linear Data Flow

Data flows linearly through pipeline stages:

```text
datasets → preprocessed → encoded → metrics → analysis
   ↓           ↓            ↓          ↓         ↓
 (raw)     (resized)   (compressed) (scores) (plots)
```

Each stage:

- Reads from previous stage
- Writes to its own directory
- Can be run independently
- Can be re-run without affecting others

### 4. Extensibility

The structure supports future pipeline stages:

**Current:**

- Dataset fetching (`data/datasets/`)

**Planned:**

- Preprocessing (`data/preprocessed/`)
- Encoding (`data/encoded/`)
- Quality measurement (`data/metrics/`)
- Analysis (`data/analysis/`)

**Easy to add:**

- New dataset sources (add to `config/datasets.json`)
- New image formats (add encoder, update `config/encoding.json`)
- New quality metrics (update `config/quality.json`)
- New analysis types (update `config/analysis.json`)

## Configuration Architecture

### Why `config/` Directory?

Configuration files are grouped in `config/` because:

1. **Centralization**: All configs in one place
2. **Versioning**: Easy to track parameter changes
3. **Sharing**: Share configs without sharing data
4. **Validation**: JSON schemas ensure correctness
5. **Scalability**: Won't clutter project root as configs grow

### JSON Schema Validation

Each configuration file has a corresponding schema:

- `config/datasets.json` ← validated by → `config/datasets.schema.json`
- Future: `config/encoding.json` ← validated by → `config/encoding.schema.json`

**Benefits:**

- Catch errors early
- Self-documenting (schema describes valid values)
- IDE support (autocomplete, validation)
- Consistent structure

### Configuration Loading

Modules automatically load their configuration:

```python
from src.dataset import DatasetFetcher

# Automatically loads config/datasets.json
fetcher = DatasetFetcher(Path("data/datasets"))

# Or specify custom config
fetcher = DatasetFetcher(
    Path("data/datasets"),
    config_file=Path("custom/config.json")
)
```

## Data Architecture

### Why `data/` Directory?

All research data lives in `data/` because:

1. **Organization**: One place for all data
2. **Disk Management**: Easy to see space usage
3. **Cleanup**: Delete entire directory to start fresh
4. **Backup**: Easy to backup just data directory
5. **Sharing**: Share data separately from code

### Subdirectory Purpose

Each subdirectory serves a specific pipeline stage:

#### `data/datasets/`

**Purpose**: Raw, unmodified datasets from external sources

**Contents:**

- DIV2K validation (100 images)
- DIV2K training (800 images)
- Future: Flickr2K, custom datasets

**Generated by**: `scripts/fetch_dataset.py`

#### `data/preprocessed/`

**Purpose**: Images after preprocessing transformations

**Contents:**

- Resized images
- Color space conversions
- Format normalization (all → PNG or similar)

**Generated by**: `scripts/preprocess_images.py` (future)

**Why separate from datasets?**

- Preprocessing is lossy (can't go back)
- Multiple preprocessing variants possible
- Raw datasets remain pristine

#### `data/encoded/`

**Purpose**: Compressed images in target formats

**Contents:**

- JPEG at quality 10, 20, ..., 100
- WebP at quality 10, 20, ..., 100
- AVIF with various speed/quality settings
- JPEG XL at different effort levels

**Generated by**: `scripts/encode_images.py` (future)

**Structure example:**

```text
data/encoded/
├── jpeg/
│   ├── q10/
│   ├── q20/
│   └── ...
├── webp/
│   ├── q10/
│   └── ...
├── avif/
│   ├── speed0_q10/
│   └── ...
└── jxl/
```

#### `data/metrics/`

**Purpose**: Quality measurements and compression statistics

**Contents:**

- SSIMULACRA2 scores (JSON/CSV)
- Butteraugli distances
- PSNR/SSIM values
- File sizes, compression ratios
- Metadata (encoding time, parameters used)

**Generated by**: `scripts/measure_quality.py` (future)

**Format**: JSON or CSV for easy loading into Pandas

**Why separate?**

- Metrics are much smaller than images
- Can regenerate by re-measuring
- Easy to load for analysis

#### `data/analysis/`

**Purpose**: Final analysis outputs

**Contents:**

- Rate-distortion curves (PNG/SVG)
- Quality comparison plots
- Statistical summaries (CSV, JSON)
- Final reports (HTML, PDF)

**Generated by**: `scripts/analyze_results.py` (future)

**Why separate?**

- Presentations and reports
- Can regenerate from metrics
- Share with non-technical audiences

## Git Ignore Strategy

The `.gitignore` excludes all data but keeps structure:

```gitignore
data/datasets/*
!data/datasets/.gitkeep
data/preprocessed/*
!data/preprocessed/.gitkeep
# ... etc
```

This means:

- Empty directories are tracked (`.gitkeep`)
- Data files are ignored
- README files are tracked
- Structure is visible in git

## Disk Space Management

### Typical Space Usage

- `data/datasets/`: 500MB - 10GB
- `data/preprocessed/`: Similar to datasets
- `data/encoded/`: 2-5× preprocessed (multiple formats/qualities)
- `data/metrics/`: <100MB (JSON/CSV)
- `data/analysis/`: <100MB (plots)

**Total**: 20-50GB for full pipeline

### Cleanup Strategies

**Full cleanup:**

```bash
rm -rf data/*
```

**Stage-specific cleanup:**

```bash
rm -rf data/encoded/*      # Re-encode from preprocessed
rm -rf data/metrics/*      # Re-measure from encoded
rm -rf data/analysis/*     # Re-analyze from metrics
```

**Keep datasets only:**

```bash
rm -rf data/preprocessed data/encoded data/metrics data/analysis
```

## Migration from Old Structure

The old structure had:

- `datasets/` in project root
- `results/` for outputs
- `datasets.json` in project root

This was problematic:

- Mixed data and code in root
- Hard to track what was data vs. code
- No clear pipeline organization

The new structure fixes this:

- All data in `data/`
- All configs in `config/`
- Clear pipeline stages
- Scalable to more stages

## Future Enhancements

### Pipeline Configuration File

Create `config/pipeline.json` to define the entire pipeline:

```json
{
  "stages": [
    {
      "name": "fetch",
      "config": "config/datasets.json",
      "output": "data/datasets/"
    },
    {
      "name": "preprocess",
      "config": "config/preprocessing.json",
      "input": "data/datasets/",
      "output": "data/preprocessed/"
    },
    {
      "name": "encode",
      "config": "config/encoding.json",
      "input": "data/preprocessed/",
      "output": "data/encoded/"
    }
  ]
}
```

This would enable:

- One-command pipeline execution
- Stage dependency tracking
- Automatic cache invalidation
- Parallel execution of independent stages

### Metadata Tracking

Add `data/.metadata/` to track:

- What datasets were used
- When each stage was run
- What parameters were used
- Pipeline provenance

This supports:

- Reproducibility
- Experiment tracking
- Result comparison

## Summary

The new architecture provides:

✅ **Clear separation** of data, code, and configuration
✅ **Scalable** to additional pipeline stages
✅ **Reproducible** with version-controlled configs
✅ **Maintainable** with logical organization
✅ **Extensible** with JSON-driven configuration

This foundation supports the full research pipeline from dataset fetching through final analysis.
